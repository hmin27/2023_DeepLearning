{"cells":[{"cell_type":"markdown","metadata":{"id":"HbD9816c2DnM"},"source":["1. Setup"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I8A2haBB2DnT","executionInfo":{"status":"ok","timestamp":1697019571271,"user_tz":-540,"elapsed":5,"user":{"displayName":"정혜민","userId":"07033770295411989576"}},"outputId":"39381d97-63c0-459a-d83c-d2a36c1c66c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","from torch.optim.lr_scheduler import StepLR\n","from torch.utils.data import Dataset, DataLoader\n","\n","import os\n","from PIL import Image\n","import pandas as pd\n","\n","\n","# Set the device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"zbHp0zS52DnX"},"source":["2. Data Loading"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sDVNcixrxKM","executionInfo":{"status":"ok","timestamp":1697019528706,"user_tz":-540,"elapsed":16966,"user":{"displayName":"정혜민","userId":"07033770295411989576"}},"outputId":"91285953-997c-43b7-9d45-fc11046067b1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"PC6ymljB2DnY","executionInfo":{"status":"ok","timestamp":1697019535185,"user_tz":-540,"elapsed":4,"user":{"displayName":"정혜민","userId":"07033770295411989576"}}},"outputs":[],"source":["# Pre-process the data\n","\n","transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n"]},{"cell_type":"code","source":["\n","import os\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","\n","data_path = '/content/drive/MyDrive/Study/ML_TeamProject/newData_224_splited'\n","\n","# Define batch size\n","batch_size = 32\n","\n","# Create custom Dataset objects for your data\n","train_dataset = ImageFolder(\n","    os.path.join(data_path, 'train'),\n","    transform=transform\n",")\n","\n","val_dataset = ImageFolder(\n","    os.path.join(data_path, 'val'),\n","    transform=transform\n",")\n","\n","test_dataset = ImageFolder(\n","    os.path.join(data_path, 'test'),\n","    transform=transform\n",")\n","\n","# Create DataLoader objects for batching and shuffling\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","\n","\n","\n","############\n","sample_batch = next(iter(train_loader))  # Get a batch of data\n","images, labels = sample_batch  # Unpack the batch into images and labels\n","\n","# Check the size of the first image\n","first_image = images[0]  # Assuming you want to check the first image in the batch\n","image_size = first_image.shape  # Get the size of the image\n","\n","print(f\"Image size: {image_size}\")\n","\n","\n","# Image size: torch.Size([3, 224, 224])\n","# len(train_loader) : 78\n","# len(val_loader) : 7\n","# len(test_loader) : 14\n","\n"],"metadata":{"id":"rrau1zkiUIq9","executionInfo":{"status":"ok","timestamp":1697019593054,"user_tz":-540,"elapsed":9738,"user":{"displayName":"정혜민","userId":"07033770295411989576"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe0466fd-9318-454a-cd0c-2c271a277443"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Image size: torch.Size([3, 32, 32])\n"]}]},{"cell_type":"markdown","metadata":{"id":"3Ji3b13S2DnZ"},"source":["3. Define the CNN"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"yFnKchLb2DnZ","executionInfo":{"status":"ok","timestamp":1697019600626,"user_tz":-540,"elapsed":717,"user":{"displayName":"정혜민","userId":"07033770295411989576"}}},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Net(nn.Module):\n","    def __init__(self, num_classes):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.fc1 = nn.Linear(256 * 28 * 28, 512)  # Update the linear layer's input size\n","        self.fc2 = nn.Linear(512, 128)\n","        self.fc3 = nn.Linear(128, num_classes)\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = x.view(-1, 256 * 28 * 28)  # Update the size to match the feature map size\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = self.fc3(x)\n","        return x\n","\n","num_classes = 215  # Number of classes in your dataset\n","net = Net(num_classes=num_classes).to(device)\n"]},{"cell_type":"markdown","metadata":{"id":"9ahXWxaf2Dna"},"source":["4. Define the Loss function and Optimizer"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"1Zd33fHd2Dnb","executionInfo":{"status":"ok","timestamp":1697019610643,"user_tz":-540,"elapsed":4,"user":{"displayName":"정혜민","userId":"07033770295411989576"}}},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n","# Reduce LR every 10 epochs by 5%, Dynamically setting lr\n"]},{"cell_type":"markdown","metadata":{"id":"NiGPJ1RZ2Dnc"},"source":["5. Train the network"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"a6EUgyvL2Dnd","colab":{"base_uri":"https://localhost:8080/","height":410},"executionInfo":{"status":"error","timestamp":1697019641955,"user_tz":-540,"elapsed":9694,"user":{"displayName":"정혜민","userId":"07033770295411989576"}},"outputId":"31dbf389-78d8-41f1-e844-52453259fde2"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-e64ca26d4282>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-88472b783a86>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update the size to match the feature map size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 200704]' is invalid for input of size 131072"]}],"source":["\n","import torch.optim as optim\n","\n","num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","    net.train()\n","\n","    running_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data[0].to(device), data[1].to(device)  #input = image, label = class_name\n","\n","        optimizer.zero_grad()\n","\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_train += labels.size(0)\n","        correct_train += (predicted == labels).sum().item()\n","\n","        if i % 10 == 9:\n","            interval_accuracy = 100 * correct_train / (10 * train_loader.batch_size)\n","            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 10:.5f}, accuracy: {interval_accuracy:.2f}%\")\n","            running_loss = 0.0\n","            correct_train = 0\n","\n","    train_accuracy = 100 * correct_train / total_train\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss / len(train_loader)}, Training accuracy: {train_accuracy:.2f}%\")\n","\n","    # Validation loop\n","    net.eval()\n","    val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for data in val_loader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","\n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_val += labels.size(0)\n","            correct_val += (predicted == labels).sum().item()\n","\n","    val_accuracy = 100 * correct_val / total_val\n","    average_val_loss = val_loss / len(val_loader)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Validation loss: {average_val_loss:.3f}, Validation accuracy: {val_accuracy:.2f}%\")\n","\n","print('Finished Training')\n","\n","\n","#  결과 정리\n","# lr = 0.001, epoch=10\n","# [1, 10] loss: 5.38286, accuracy: 0.94%\n","# [1, 20] loss: 5.37374, accuracy: 0.62%\n","# [1, 30] loss: 5.37406, accuracy: 0.62%\n","# [1, 40] loss: 5.37909, accuracy: 0.94%\n","# [1, 50] loss: 5.37122, accuracy: 0.62%\n","\n","# lr=0.005, epoch=20\n","# [1, 10] loss: 5.37653, accuracy: 0.00%\n","# [1, 20] loss: 5.37914, accuracy: 0.00%\n","# [1, 30] loss: 5.38180, accuracy: 0.00%\n","# [1, 40] loss: 5.38372, accuracy: 0.31%\n","# [1, 50] loss: 5.39082, accuracy: 0.00%\n","# [1, 60] loss: 5.38192, accuracy: 0.31%\n","# [1, 70] loss: 5.38536, accuracy: 0.00%\n","# Epoch 1/20, Training Loss: 0.5522051896804419, Training accuracy: 0.00%\n","# Epoch 1/20, Validation loss: 5.372, Validation accuracy: 0.48%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"viJE93CQ2Dnd"},"outputs":[],"source":["torch.save(net, 'model2.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JOFj7Y_E2Dne","executionInfo":{"status":"ok","timestamp":1696850233043,"user_tz":-540,"elapsed":278,"user":{"displayName":"정혜민","userId":"07033770295411989576"}},"outputId":"b3fccf88-3e22-47c9-ed24-9dd0aadbbc34"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=400, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")"]},"metadata":{},"execution_count":9}],"source":["# Assuming the model's class is `Net` as in previous examples\n","model = torch.load('model2.pth')\n","model.eval()  # Set the model to evaluation mode\n"]},{"cell_type":"markdown","metadata":{"id":"ax22YktU2Dne"},"source":["6. Test the network on test data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2OiH4RfH2Dnf","executionInfo":{"status":"ok","timestamp":1696850248767,"user_tz":-540,"elapsed":9174,"user":{"displayName":"정혜민","userId":"07033770295411989576"}},"outputId":"7481d4f1-a140-40ff-d473-33699f46daa0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the network on the 10000 test images: 58 %\n"]}],"source":["correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data[0].to(device), data[1].to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Xt1y0gcLJ92S"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"alpha","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}